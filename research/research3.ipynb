{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV74djcDAxQU",
        "outputId": "d5821bc6-072f-4b76-b6f3-d61336d49199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed training data shape: (20384, 28)\n",
            "Transformed test data shape: (5096, 28)\n",
            "Training Logistic Regression...\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.7433\n",
            "Precision: 0.7647\n",
            "Recall: 0.8889\n",
            "F1 Score: 0.8221\n",
            "ROC AUC: 0.7798\n",
            "-------------------------------------\n",
            "Training Random Forest...\n",
            "Model: Random Forest\n",
            "Accuracy: 0.7265\n",
            "Precision: 0.7732\n",
            "Recall: 0.835\n",
            "F1 Score: 0.8029\n",
            "ROC AUC: 0.762\n",
            "-------------------------------------\n",
            "Training Gradient Boosting...\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.7551\n",
            "Precision: 0.7803\n",
            "Recall: 0.8812\n",
            "F1 Score: 0.8277\n",
            "ROC AUC: 0.7913\n",
            "-------------------------------------\n",
            "Training SVM...\n",
            "Model: SVM\n",
            "Accuracy: 0.7571\n",
            "Precision: 0.7813\n",
            "Recall: 0.8833\n",
            "F1 Score: 0.8291\n",
            "ROC AUC: 0.7655\n",
            "-------------------------------------\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:02:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:02:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: XGBoost\n",
            "Accuracy: 0.742\n",
            "Precision: 0.7753\n",
            "Recall: 0.8636\n",
            "F1 Score: 0.8171\n",
            "ROC AUC: 0.7803\n",
            "-------------------------------------\n",
            "Summary of model performances:\n",
            "                 model  accuracy  precision    recall        f1   roc_auc\n",
            "0  Logistic Regression  0.743328   0.764736  0.888856  0.822138  0.779775\n",
            "1        Random Forest  0.726452   0.773210  0.835049  0.802940  0.762050\n",
            "2    Gradient Boosting  0.755102   0.780266  0.881211  0.827672  0.791329\n",
            "3                  SVM  0.757064   0.781274  0.883270  0.829147  0.765482\n",
            "4              XGBoost  0.741954   0.775343  0.863570  0.817082  0.780282\n",
            "Best model based on accuracy: SVM with accuracy 0.7571\n",
            "Best model (SVM) saved as SVM_best_model.pkl\n",
            "Preprocessor saved as preprocessor.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:02:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "df = pd.read_csv('EasyVisa.csv')\n",
        "\n",
        "# Define target variable as 1 for 'Certified' and 0 otherwise\n",
        "\n",
        "y = (df['case_status'] == 'Certified').astype(int)\n",
        "\n",
        "# Define features (dropping case_status and case_id)\n",
        "X = df.drop(['case_status', 'case_id'], axis=1)\n",
        "\n",
        "# Split categorical and numerical features\n",
        "categorical_features = ['continent', 'education_of_employee', 'has_job_experience',\n",
        "                        'requires_job_training', 'region_of_employment',\n",
        "                        'unit_of_wage', 'full_time_position']\n",
        "numerical_features = ['no_of_employees', 'yr_of_estab', 'prevailing_wage']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create preprocessing pipelines\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"Transformed training data shape: \" + str(X_train_transformed.shape))\n",
        "print(\"Transformed test data shape: \" + str(X_test_transformed.shape))\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    try:\n",
        "        y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "    except:\n",
        "        try:\n",
        "            y_pred_proba = model.decision_function(X_test)\n",
        "        except:\n",
        "            y_pred_proba = y_pred\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print('Model: ' + model_name)\n",
        "    print('Accuracy: ' + str(round(accuracy,4)))\n",
        "    print('Precision: ' + str(round(precision,4)))\n",
        "    print('Recall: ' + str(round(recall,4)))\n",
        "    print('F1 Score: ' + str(round(f1,4)))\n",
        "    print('ROC AUC: ' + str(round(roc_auc,4)))\n",
        "    print('-------------------------------------')\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc\n",
        "    }\n",
        "\n",
        "# List to store model metrics\n",
        "model_metrics = []\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print('\\\n",
        "Training Logistic Regression...')\n",
        "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_model.fit(X_train_transformed, y_train)\n",
        "metrics_log = evaluate_model(log_model, X_test_transformed, y_test, 'Logistic Regression')\n",
        "model_metrics.append(metrics_log)\n",
        "\n",
        "# 2. Random Forest\n",
        "print('\\\n",
        "Training Random Forest...')\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_model.fit(X_train_transformed, y_train)\n",
        "metrics_rf = evaluate_model(rf_model, X_test_transformed, y_test, 'Random Forest')\n",
        "model_metrics.append(metrics_rf)\n",
        "\n",
        "# 3. Gradient Boosting\n",
        "print('\\\n",
        "Training Gradient Boosting...')\n",
        "gb_model = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
        "gb_model.fit(X_train_transformed, y_train)\n",
        "metrics_gb = evaluate_model(gb_model, X_test_transformed, y_test, 'Gradient Boosting')\n",
        "model_metrics.append(metrics_gb)\n",
        "\n",
        "# 4. SVM\n",
        "print('\\\n",
        "Training SVM...')\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "svm_model.fit(X_train_transformed, y_train)\n",
        "metrics_svm = evaluate_model(svm_model, X_test_transformed, y_test, 'SVM')\n",
        "model_metrics.append(metrics_svm)\n",
        "\n",
        "# 5. XGBoost\n",
        "print('\\\n",
        "Training XGBoost...')\n",
        "xgb_model = xgb.XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train_transformed, y_train)\n",
        "metrics_xgb = evaluate_model(xgb_model, X_test_transformed, y_test, 'XGBoost')\n",
        "model_metrics.append(metrics_xgb)\n",
        "\n",
        "# Compare results\n",
        "metrics_df = pd.DataFrame(model_metrics)\n",
        "print('\\\n",
        "Summary of model performances:')\n",
        "print(metrics_df)\n",
        "\n",
        "# Find the best model based on accuracy\n",
        "best_model_idx = metrics_df['accuracy'].idxmax()\n",
        "best_model_name = metrics_df.loc[best_model_idx, 'model']\n",
        "best_accuracy = metrics_df.loc[best_model_idx, 'accuracy']\n",
        "print('\\\n",
        "Best model based on accuracy: ' + best_model_name + ' with accuracy ' + str(round(best_accuracy,4)))\n",
        "\n",
        "# Save the best model and preprocessor\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    best_model = log_model\n",
        "elif best_model_name == 'Random Forest':\n",
        "    best_model = rf_model\n",
        "elif best_model_name == 'Gradient Boosting':\n",
        "    best_model = gb_model\n",
        "elif best_model_name == 'SVM':\n",
        "    best_model = svm_model\n",
        "else:\n",
        "    best_model = xgb_model\n",
        "\n",
        "model_name = best_model_name.replace(' ', '_')+\"_best_model.pkl\"\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "  pickle.dump(best_model, f)\n",
        "with open('preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "\n",
        "print('\\\n",
        "Best model (' + best_model_name + ') saved as ' + model_name)\n",
        "print('Preprocessor saved as preprocessor.pkl')\n",
        "\n",
        "# For tree-based models, show feature importances if available\n",
        "if best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
        "    # Get feature names from preprocessor\n",
        "    cat_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "    all_features = np.concatenate([numerical_features, cat_features])\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = best_model.feature_importances_\n",
        "\n",
        "    # Create a DataFrame for feature importances\n",
        "    feature_importances = pd.DataFrame({\n",
        "        'Feature': all_features,\n",
        "        'Importance': importances\n",
        "    })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxmXAJ2zAxM2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PQlp401ClO-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
